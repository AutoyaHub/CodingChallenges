{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# load the json file\n",
    "with open(\"label_to_txt.json\", \"r\") as file:\n",
    "    label_to_txt = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/marcsperzel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class TextQueryEngine:\n",
    "    def __init__(self):\n",
    "        self.documents = []  # List to store the original documents\n",
    "        self.labels = []  # List to store the corresponding labels for the documents\n",
    "        self.vectorizer = TfidfVectorizer()  # TF-IDF vectorizer for text representation\n",
    "        self.stopwords = set(stopwords.words('english'))  # Set of stopwords for text preprocessing\n",
    "        self.lemmatizer = WordNetLemmatizer()  # Lemmatizer for word lemmatization\n",
    "\n",
    "    def preprocess_text(self, text):\n",
    "        tokens = word_tokenize(text.lower())  # Tokenize the text into words and convert to lowercase\n",
    "        tokens = [self.lemmatizer.lemmatize(token) for token in tokens if token.isalnum()]  # Lemmatize each word if it is alphanumeric\n",
    "        tokens = [token for token in tokens if token not in self.stopwords]  # Remove stopwords from the tokens\n",
    "        return ' '.join(tokens)  # Return the preprocessed text as a string\n",
    "\n",
    "    def build_index(self, documents, labels):\n",
    "        self.documents = documents  # Store the original documents\n",
    "        self.labels = labels  # Store the corresponding labels\n",
    "        preprocessed_documents = [self.preprocess_text(doc) for doc in documents]  # Preprocess each document\n",
    "        self.vectorizer.fit_transform(preprocessed_documents)  # Fit the vectorizer on the preprocessed documents\n",
    "\n",
    "    def query(self, query_text):\n",
    "        preprocessed_query = self.preprocess_text(query_text)  # Preprocess the query text\n",
    "        query_vector = self.vectorizer.transform([preprocessed_query])  # Transform the preprocessed query text into a vector\n",
    "        similarities = cosine_similarity(query_vector, self.vectorizer.transform(self.documents))  # Calculate cosine similarity between the query vector and document vectors\n",
    "        ranked_indices = similarities.argsort()[0][::-1]  # Sort indices in descending order of similarity\n",
    "        ranked_documents = [self.documents[index] for index in ranked_indices]  # Retrieve the ranked documents based on the sorted indices\n",
    "        ranked_labels = [self.labels[index] for index in ranked_indices]  # Retrieve the corresponding labels for the ranked documents\n",
    "        results = []  # List to store the results\n",
    "        # unique_labels = set(ranked_labels)  # Get unique labels (Resets ordering)\n",
    "        for label in ranked_labels:\n",
    "            label_indices = [i for i, l in enumerate(ranked_labels) if l == label]  # Find indices of documents with the current label\n",
    "            label_documents = [ranked_documents[i] for i in label_indices]  # Get documents corresponding to the label\n",
    "            results.append(label_documents)  # Append the documents to the results list\n",
    "        return results  # Return the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqe = TextQueryEngine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tqe.build_index(list(label_to_txt.values()), list(label_to_txt.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {label: tqe.query(label) for label in tqe.labels}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feedback:\n",
    "\n",
    "- No type information (-)\n",
    "- Class (+)\n",
    "- Notes (+)\n",
    "- Pythonic (+)\n",
    "- Output is wrong (-)\n",
    "- Idea is correct (+)\n",
    "\n",
    "Mistakes:\n",
    "\n",
    "Applying `set(ranked_labels)` resets the order of the labels and thus the order of the predictions. This is why the output is wrong. The idea is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nomos_v2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
